{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, RandomHorizontalFlip\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "# Set Device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.has_mps:\n",
    "    device = 'mps'\n",
    "torch.manual_seed(237237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the Data\n",
    "transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "painting_transform = Compose([Resize((256, 256)), ToTensor(), RandomHorizontalFlip()])\n",
    "photo_dataset = ImageFolder(r'../data/photo_jpg', transform=transform)\n",
    "monet_dataset = ImageFolder(r'../data/monet_jpg', transform=painting_transform)\n",
    "\n",
    "train_size = int(0.8 * len(photo_dataset))\n",
    "val_size = len(photo_dataset) - train_size\n",
    "photo_train, photo_val = random_split(photo_dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 16\n",
    "photo_loader_train = DataLoader(photo_train, batch_size=batch_size, shuffle=True)\n",
    "photo_loader_val = DataLoader(photo_val, batch_size=batch_size, shuffle=False)\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9560676868603744"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display Monet Images and Input Photos\n",
    "def show_images(loader, title):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, (images, _) in enumerate(loader):\n",
    "        images = images.numpy().transpose((0, 2, 3, 1))\n",
    "        for j in range(5):\n",
    "            plt.subplot(1, 5, j+1)\n",
    "            plt.imshow(images[j])\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(title)\n",
    "        break\n",
    "    plt.show()\n",
    "\n",
    "# Display images from each dataset\n",
    "show_images(photo_loader_val, 'Input Images')\n",
    "show_images(monet_loader, 'Monet Paintings')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c597816d338a673b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to denormalize image for display\n",
    "def denormalize(image):\n",
    "    image = image * 0.5 + 0.5  # Assuming images were normalized in range [-1, 1]\n",
    "    return image.clamp(0, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "64163e2c6dde20e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to display images\n",
    "def show_transformed_images(generator, photo_loader, device, denormalize_func):\n",
    "    generator.eval()  # Set the generator to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for i, (photos, _) in enumerate(photo_loader):\n",
    "            if i >= 5:  # Display first 5 images\n",
    "                break\n",
    "\n",
    "            original_photo = photos.to(device)\n",
    "            transformed_image = generator(original_photo).cpu()\n",
    "\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.imshow(denormalize_func(original_photo[0].cpu()).permute(1, 2, 0).numpy())\n",
    "\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Transformed Image\")\n",
    "            plt.imshow(denormalize_func(transformed_image[0]).permute(1, 2, 0).numpy())\n",
    "\n",
    "            plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1b5a2b568b0af26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input size: (3 x 256 x 256)\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # State size: (64 x 128 x 128)\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # State size: (128 x 64 x 64)\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # State size: (256 x 32 x 32)\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # State size: (512 x 16 x 16)\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (256 x 32 x 32)\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (128 x 64 x 64)\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (64 x 128 x 128)\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output state size: (3 x 256 x 256)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4381cb38d4e2768f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        # Input size: (3 x 256 x 256)\n",
    "        nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.MaxPool2d(2, 2),  # Max pooling\n",
    "        # State size: (64 x 64 x 64)\n",
    "        nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(128),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        nn.MaxPool2d(2, 2),  # Max pooling\n",
    "        # State size: (128 x 16 x 16)\n",
    "        nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(256),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # State size: (256 x 8 x 8)\n",
    "        nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(512),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # State size: (512 x 4 x 4)\n",
    "        nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "        nn.BatchNorm2d(1024),\n",
    "        nn.LeakyReLU(0.2, inplace=True),\n",
    "        # State size: (1024 x 2 x 2)\n",
    "        nn.Conv2d(1024, 1, 2, 1, 0, bias=False),  # Adjusted final convolution\n",
    "        nn.Sigmoid()\n",
    "        # Output state size: (1 x 1 x 1)\n",
    "    )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d151ae8a0a0d0f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1f19b63f4d7b4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Loss Functions and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30c34e0c3cbdb99c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    monet_loader_iter = itertools.cycle(monet_loader)\n",
    "    \n",
    "    for i, photo_data in enumerate(photo_loader_train):\n",
    "        monet_data = next(monet_loader_iter)\n",
    "\n",
    "        current_batch_size = photo_data[0].size(0)\n",
    "\n",
    "        photos = photo_data[0].to(device)\n",
    "        monets = monet_data[0].to(device)\n",
    "\n",
    "        real_label = torch.ones(current_batch_size, 1, device=device)\n",
    "        fake_label = torch.zeros(current_batch_size, 1, device=device)\n",
    "\n",
    "        # === Discriminator Training ===\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        output = discriminator(photos).view(-1)\n",
    "        lossD_real = criterion(output, real_label.view(-1))\n",
    "        lossD_real.backward()\n",
    "\n",
    "        fake_monets = generator(photos)\n",
    "        output = discriminator(fake_monets.detach()).view(-1)\n",
    "        lossD_fake = criterion(output, fake_label.view(-1))\n",
    "        lossD_fake.backward()\n",
    "\n",
    "        lossD = lossD_real + lossD_fake\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # === Generator Training ===\n",
    "        optimizer_G.zero_grad()\n",
    "        output = discriminator(fake_monets).view(-1)\n",
    "        lossG = criterion(output, real_label.view(-1))\n",
    "        lossG.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(photo_loader_train)}], D Loss: {lossD.item():.4f}, G Loss: {lossG.item():.4f}')\n",
    "\n",
    "    # Validation\n",
    "    for photo_data in photo_loader_val:\n",
    "        monet_data = next(monet_loader_iter)\n",
    "        with torch.no_grad():\n",
    "            photos = photo_data[0].to(device)\n",
    "            monets = monet_data[0].to(device)\n",
    "            fake_monets = generator(photos)\n",
    "\n",
    "            current_batch_size = photo_data[0].size(0)\n",
    "            real_labels = torch.ones(monets.size(0), 1).to(device)\n",
    "            fake_labels = torch.zeros(fake_monets.size(0), 1).to(device)\n",
    "            combined_images = torch.cat([monets, fake_monets], dim=0)\n",
    "            combined_labels = torch.cat([real_labels, fake_labels], dim=0)\n",
    "\n",
    "            output = discriminator(combined_images).view(-1)\n",
    "            loss_val = criterion(output, combined_labels.view(-1))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Validation Error: {loss_val.item():.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch < 10:\n",
    "        show_transformed_images(generator, photo_loader_val, device, denormalize)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a910aa78d913bd00"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display Original and Transformed Images\n",
    "generator.eval()\n",
    "\n",
    "# Display first 5 images\n",
    "for i, (photo_data, _) in enumerate(photo_loader_val):\n",
    "    if i >= 5:  # Only display first 5 images\n",
    "        break\n",
    "    \n",
    "    # Original image\n",
    "    original_image = photo_data[0]\n",
    "\n",
    "    # Generate transformed image\n",
    "    with torch.no_grad():\n",
    "        transformed_image = generator(original_image.to(device)).cpu()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(denormalize(original_image[0]).permute(1, 2, 0))  # Convert from CxHxW to HxWxC\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Transformed Image\")\n",
    "    plt.imshow(denormalize(transformed_image[0]).permute(1, 2, 0))  # Convert from CxHxW to HxWxC\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0cb8ba6940a9191"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(generator.state_dict(), 'generator.ckpt')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.ckpt')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3357982485eedae4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
