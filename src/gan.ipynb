{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:49:50.838218600Z",
     "start_time": "2023-11-20T22:49:50.817551800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1b7b80a5f10>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Set Device\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.has_mps:\n",
    "    device = 'mps'\n",
    "torch.manual_seed(237237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Get the Data\n",
    "transform = Compose([Resize((256, 256)), ToTensor()])\n",
    "photo_dataset = ImageFolder(r'../data', transform=transform)\n",
    "monet_dataset = ImageFolder(r'../data', transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "photo_loader = DataLoader(photo_dataset, batch_size=batch_size, shuffle=True)\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:49:50.889692300Z",
     "start_time": "2023-11-20T22:49:50.822593300Z"
    }
   },
   "id": "b6a5aa0e4f06e92a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input is a latent vector Z\n",
    "            nn.ConvTranspose2d(100, 1024, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (1024 x 4 x 4)\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (512 x 8 x 8)\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (256 x 16 x 16)\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (128 x 32 x 32)\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            # State size: (64 x 64 x 64)\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output state size: (3 x 128 x 128)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:49:50.906165800Z",
     "start_time": "2023-11-20T22:49:50.897069500Z"
    }
   },
   "id": "9267b74e1c9d812c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input size: (3 x 256 x 256)\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (64 x 128 x 128)\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (128 x 64 x 64)\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (256 x 32 x 32)\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (512 x 16 x 16)\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State size: (1024 x 8 x 8)\n",
    "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False),\n",
    "            nn.Flatten(),\n",
    "            nn.Sigmoid()\n",
    "            # Output state size: (1 x 1 x 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T23:07:27.467421800Z",
     "start_time": "2023-11-20T23:07:27.467421800Z"
    }
   },
   "id": "2d87107bbf2682c4"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Initialize the models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T23:07:29.772728900Z",
     "start_time": "2023-11-20T23:07:29.612068900Z"
    }
   },
   "id": "3e69a0a9d2923996"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# Define Loss Functions and Optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T23:07:31.412944800Z",
     "start_time": "2023-11-20T23:07:31.399515600Z"
    }
   },
   "id": "d1217ac1879f2a17"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Output Shape: torch.Size([800])\n",
      "label shape torch.Size([32, 1])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([32])) that is different to the input size (torch.Size([800])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 23\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDiscriminator Output Shape:\u001B[39m\u001B[38;5;124m\"\u001B[39m, output\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel shape\u001B[39m\u001B[38;5;124m\"\u001B[39m, real_label\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m---> 23\u001B[0m lossD_real \u001B[38;5;241m=\u001B[39m \u001B[43mcriterion\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_label\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m lossD_real\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Train with fake images\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001B[0m, in \u001B[0;36mBCELoss.forward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m    617\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 618\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3113\u001B[0m, in \u001B[0;36mbinary_cross_entropy\u001B[1;34m(input, target, weight, size_average, reduce, reduction)\u001B[0m\n\u001B[0;32m   3111\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[0;32m   3112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize():\n\u001B[1;32m-> 3113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   3114\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing a target size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) that is different to the input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) is deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3115\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease ensure they have the same size.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m   3116\u001B[0m     )\n\u001B[0;32m   3118\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3119\u001B[0m     new_size \u001B[38;5;241m=\u001B[39m _infer_size(target\u001B[38;5;241m.\u001B[39msize(), weight\u001B[38;5;241m.\u001B[39msize())\n",
      "\u001B[1;31mValueError\u001B[0m: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([800])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (photo_data, monet_data) in enumerate(zip(photo_loader, monet_loader)):\n",
    "        # get size of current batch\n",
    "        current_batch_size = photo_data[0].size(0)\n",
    "        \n",
    "        # Get images and move them to device\n",
    "        photos = photo_data[0].to(device)\n",
    "        monets = monet_data[0].to(device)\n",
    "        \n",
    "        # Correctly sized labels\n",
    "        real_label = torch.ones(current_batch_size, 1, device=device)\n",
    "        fake_label = torch.zeros(current_batch_size, 1, device=device)\n",
    "        \n",
    "        # === Discriminator Training ===\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Train with real images\n",
    "        output = discriminator(photos).view(-1)\n",
    "        print(\"Discriminator Output Shape:\", output.shape)\n",
    "        print(\"label shape\", real_label.shape)\n",
    "        lossD_real = criterion(output, real_label.view(-1))\n",
    "        lossD_real.backward()\n",
    "        \n",
    "        # Train with fake images\n",
    "        fake_monets = generator(photos)\n",
    "        output = discriminator(fake_monets.detach()).view(-1)\n",
    "        lossD_fake = criterion(output, fake_label.view(-1))\n",
    "        lossD_fake.backward()\n",
    "        \n",
    "        # Update Discriminator\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # === Train Generator ===\n",
    "        optimizer_G.zero_grad()\n",
    "        output = discriminator(fake_monets).view(-1)\n",
    "        lossG = criterion(output, real_label)\n",
    "        lossG.backward()\n",
    "        \n",
    "        # Update Generator\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Display Progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(photo_loader)}]')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T23:09:29.314297800Z",
     "start_time": "2023-11-20T23:09:28.742932Z"
    }
   },
   "id": "419f389da1aa5533"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to denormalize image for display\n",
    "def denormalize(image):\n",
    "    image = image * 0.5 + 0.5  # Assuming images were normalized in range [-1, 1]\n",
    "    return image.clamp(0, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T22:49:59.292124900Z",
     "start_time": "2023-11-20T22:49:59.292124900Z"
    }
   },
   "id": "ff4ed123f4b3655c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display Original and Transformed Images\n",
    "generator.eval()\n",
    "\n",
    "# Display first 5 images\n",
    "for i, (photo_data, _) in enumerate(photo_loader):\n",
    "    if i >= 5:  # Only display first 5 images\n",
    "        break\n",
    "    \n",
    "    # Original image\n",
    "    original_image = photo_data[0]\n",
    "\n",
    "    # Generate transformed image\n",
    "    with torch.no_grad():\n",
    "        transformed_image = generator(original_image.to(device)).cpu()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.imshow(denormalize(original_image[0]).permute(1, 2, 0))  # Convert from CxHxW to HxWxC\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Transformed Image\")\n",
    "    plt.imshow(denormalize(transformed_image[0]).permute(1, 2, 0))  # Convert from CxHxW to HxWxC\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-20T22:49:59.292124900Z"
    }
   },
   "id": "53e075fe9b5000b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(generator.state_dict(), 'generator.ckpt')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.ckpt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-20T22:49:59.292124900Z"
    }
   },
   "id": "d711d7c6cbb0db3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
